import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
#LOADING DATASET
df=pd.read_csv('HR_Attrition.csv')
# WE CAN GET HEAD - FIRST 5
df.head()
#WE CAN GET TAIL - MEANS LAST 5
df.tail()
#WE CAN GET NUMBER OF ROWS AND COLUMNS IN DATA
df.shape
#WE CAN GET NUMBER OF COLUMNS 
df.columns
# STATISTICAL VIEW ( MEAN, MODE, MEDIAN ETC)
df.describe()
#TO COUNT THE NULL VALUES (EMPTY VALUES)
df.isnull().sum()

#TO CHECK THE DATA TYPE
df.dtypes
#Get the values from attriation column from dataset
df['Attrition'].value_counts()
#TO CHECK THE STATISTICAL VIEW (T) REFERS TO TRANSPOSE 
df.describe().T
#UNIQUE ITEMS NUMBERS OF EVERY COLUMN
df.nunique()
#to check if duplicate data is present
duplicate_data= df[df.duplicated()]
duplicate_data
EXPLORATORY DATA ANALYSIS :
attrition_count = pd.DataFrame(df['Attrition'].value_counts())
attrition_count
#VISUALIZATION USING PIE CHART TO CHECK PROBABLITY

plt.pie(attrition_count['Attrition'],labels =attrition_count.to_string(header=False).replace(" ","i").split('\n'),
       explode=(0.1,0),startangle=30,autopct= '%1.1f%%', shadow=True,
       )
plt.title('Employee Attrition Probablity')
#VISUALIZATION USING COUNT PLOT

sns.countplot(df['Attrition'])
#FREQUENCY DISTRUBTION FOR CATAGEORICAL VALUES TO ATTRITION

f, ax=plt.subplots(4,2, figsize=(20,15))
ax[0,0]=sns.countplot(x='Attrition',hue='EducationField',data=df, ax= ax[0,0], palette='Set1')
ax[0,0].set_title("Frequency Distribution of Education Field")

ax[1,0]=sns.countplot(x='Attrition',hue='Department',data=df, ax= ax[1,0], palette='Set1')
ax[1,0].set_title("Frequency Distribution of Department")

ax[0,1]=sns.countplot(x='Attrition',hue='Education',data=df, ax= ax[0,1], palette='Set1')
ax[0,1].set_title("Frequency Distribution of Education ")

ax[1,1]=sns.countplot(x='Attrition',hue='BusinessTravel',data=df, ax= ax[1,1], palette='Set1')
ax[1,1].set_title("Frequency Distribution of Business Travel")

ax[2,0]=sns.countplot(x='Attrition',hue='JobRole',data=df, ax= ax[2,0], palette='Set1')
ax[2,0].set_title("Frequency Distribution of Job Role")

ax[2,1]=sns.countplot(x='Attrition',hue='OverTime',data=df, ax= ax[2,1], palette='Set1')
ax[2,1].set_title("Frequency Distribution of Over Time")

ax[3,0]=sns.countplot(x='Attrition',hue='EnvironmentSatisfaction',data=df, ax= ax[3,0], palette='Set1')
ax[3,0].set_title("Frequency Distribution of Environment Satisfaction")

ax[3,1]=sns.countplot(x='Attrition',hue='WorkLifeBalance',data=df, ax= ax[3,1], palette='Set1')
ax[3,1].set_title("Frequency Distribution of Work Life Balance")

f.tight_layout()
#EMPLOYES THAT LEFT / STAYED AS PER THE AGE

fig_dims= (12,6)
fig, ax = plt.subplots(figsize=fig_dims)

sns.countplot(x='Age',hue='Attrition',data= df, palette='colorblind')
plt.subplots(figsize=(12,6))
sns.countplot(x='YearsAtCompany', hue='Attrition', data=df, palette='colorblind')
#DATA TYPES AND VALUES

for column in df.columns:
    if df[column].dtype==object:
        print(str(column)+':'+str(df[column].unique()))
        print(df[column].value_counts())
        print('------------*----------*----------*---------*----------')
# TO DROP UNWANTED COLUMNS WHICH ARE NOT NEEDED

df=df.drop('Over18',axis=1)
df=df.drop('EmployeeCount',axis=1)
df=df.drop('EmployeeNumber',axis=1)
df=df.drop('StandardHours',axis=1)
# CORRELATIONS OF THE COLUMNS

df.corr()
#VISUALIZE THE CORRELATION USING HEATMAP -

plt.figure(figsize=(15,15))
sns.heatmap(df.corr(), annot=True, fmt= '.0%', cmap='YlGnBu',cbar=False)
DATA PREPROCESSING: 

# TRANSFORM FROM NON NUMERICAL COLUMNS INTO NUMERICAL COLUMNS

from sklearn.preprocessing import LabelEncoder
for column in df.columns:
    if df[column].dtype==np.number:
        continue
    df[column]=LabelEncoder().fit_transform(df[column])

# WE CREATED A NEW COLUMN 

df['Age_Years'] = df['Age']

# WE DROP THE AGE COLUMN

df=df.drop('Age',axis=1)

#SPLIT THE DATASET INTO INDEPENDENT 'X' & DEPENDENT 'Y' VARIABLES
x=df.iloc[:, 1:df.shape[1]].values
y=df.iloc[:, 0].values
# SPLIT THE DATASET INTO 75% TRAINING AND 25% TESTING
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test =train_test_split(x,y,test_size=0.25,random_state=101)

# USE RFC ALOGORITHM 
from sklearn.ensemble import RandomForestClassifier
forest= RandomForestClassifier(n_estimators=10, criterion='entropy',random_state=101)
forest.fit(x_train, y_train)
y_pred=forest.predict(x_test)
#GET THE ACCURACY OF TRAINING DATA
forest.score(x_train, y_train)
#HERE'S THE ACCURACY USING ABSOLUTE SCORE *100
score=forest.score(x_train, y_train)
print('Random Forest Classification :', np.abs(score)*100)

#SHOW THE CONFUSION MATRIX NAD ACCURACY TO TEST MODEL
from sklearn.metrics import confusion_matrix
cm= confusion_matrix(y_test,forest.predict(x_test))
TN = cm[0][0]
TP = cm[1][1]
FN = cm[1][0]
FP = cm[0][1]

print(cm)
print('Model Testing Accuracy is:', (TP+TN)/(TN+TP+FN+FP))

#misclassification rate  tells us the percentage of observations incorrectly predicted by model
print('Misclassification Rate:',(FP+FN)/(TN+TP+FN+FP))

#ratio of correctly classified positive samples 
print('Precission Rate:',(TP)/(TP+FP))

# fraction of the relevant documents that are successfully retrieved 
print('Recall Rate:',(TP)/(TP+FN))















